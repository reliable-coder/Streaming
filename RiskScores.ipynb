{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import os\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'\n# !pip install pyspark",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.classification import LogisticRegression\nimport json",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "def create_spark_session():\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"SimpleFraudDetection\") \\\n        .getOrCreate()\n    return spark",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def process_stream(spark, kafka_topic):\n    df = spark \\\n        .readStream \\\n        .format(\"kafka\") \\\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n        .option(\"subscribe\", kafka_topic) \\\n        .option(\"startingOffsets\",    \"earliest\")    \\\n        .load()\n\n    # Transaction data in JSON format for example\n    df = df.selectExpr(\"CAST(value AS STRING)\") \\\n           .select(from_json(\"value\", schema).alias(\"data\"))\n\n    # Join with customer risk scores from parquet\n    customer_risk_scores = spark.read.parquet(\"customer_risk_scores\")\n    df = df.join(customer_risk_scores, \"customer_id\")\n\n    # Feature engineering Sample\n    df = df.withColumn(\"amount_bin\", col(\"amount\").cast(\"int\") / 100)\n    df = df.withColumn(\"time_diff\", unix_timestamp() - col(\"timestamp\"))\n\n    # Load pre-trained model (update it with your own model)\n    model = LogisticRegression.load(\"fraud_model\")\n\n    # Make predictions (job)\n    predictions = model.transform(df)\n\n    # Filter fraudulent transactions\n    fraud_transactions = predictions.filter(col(\"prediction\") === 1)\n\n    # Update customer risk scores (batch process)\n    # my batch process here\n\n    # Write fraudulent transactions to a sink (e.g., Kafka)\n    fraud_transactions.writeStream \\\n        .format(\"kafka\") \\\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n        .option(\"topic\", \"fraud_alerts\") \\\n        .start()\n\n    fraud_transactions.awaitTermination()\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "if __name__ == \"__main__\":\n    spark = create_spark_session()\n    process_stream(spark, \"transactions\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}